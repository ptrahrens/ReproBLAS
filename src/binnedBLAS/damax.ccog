#include <stdlib.h>
#include <math.h>

#include "../config.h"
#include "../common/common.h"
#include "binnedBLAS.h"

/*[[[cog
import cog
import generate
import dataTypes
import amax
import vectorizations

code_block = generate.CodeBlock()
vectorizations.conditionally_include_vectorizations(code_block)
cog.out(str(code_block))
]]]*/
#if (defined(__AVX__) && !defined(reproBLAS_no__AVX__))
  #include <immintrin.h>

#elif (defined(__SSE2__) && !defined(reproBLAS_no__SSE2__))
  #include <emmintrin.h>

#else


#endif
//[[[end]]]

/**
 * @internal
 * @brief  Find maximum absolute value in vector of double precision
 *
 * Returns the absolute value of the element of maximum absolute value in an array.
 *
 * @param N vector length
 * @param X double precision vector
 * @param incX X vector stride (use every incX'th element)
 * @return absolute maximum value of X
 *
 * @author Willow Ahrens
 * @date   15 Jan 2016
 */
double binnedBLAS_damax(const int N, const double *X, const int incX){
  double amax;
  /*[[[cog
  cog.out(generate.generate(amax.AMax(dataTypes.Double, "N", "X", "incX", "(&amax)"), cog.inFile, args, params, mode))
  ]]]*/
  #if (defined(__AVX__) && !defined(reproBLAS_no__AVX__))
    __m256d abs_mask_tmp;
    {
      __m256d tmp;
      tmp = _mm256_set1_pd(1);
      abs_mask_tmp = _mm256_set1_pd(-1);
      abs_mask_tmp = _mm256_xor_pd(abs_mask_tmp, tmp);
      tmp = _mm256_cmp_pd(tmp, tmp, 0);
      abs_mask_tmp = _mm256_xor_pd(abs_mask_tmp, tmp);
    }
    double max_buffer_tmp[4] __attribute__((aligned(32))); (void)max_buffer_tmp;

    int i;

    __m256d X_0, X_1, X_2, X_3, X_4, X_5, X_6, X_7;
    __m256d m_0;
    m_0 = _mm256_setzero_pd();

    if(incX == 1){

      for(i = 0; i + 32 <= N; i += 32, X += 32){
        X_0 = _mm256_and_pd(_mm256_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_loadu_pd(X + 4), abs_mask_tmp);
        X_2 = _mm256_and_pd(_mm256_loadu_pd(X + 8), abs_mask_tmp);
        X_3 = _mm256_and_pd(_mm256_loadu_pd(X + 12), abs_mask_tmp);
        X_4 = _mm256_and_pd(_mm256_loadu_pd(X + 16), abs_mask_tmp);
        X_5 = _mm256_and_pd(_mm256_loadu_pd(X + 20), abs_mask_tmp);
        X_6 = _mm256_and_pd(_mm256_loadu_pd(X + 24), abs_mask_tmp);
        X_7 = _mm256_and_pd(_mm256_loadu_pd(X + 28), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        m_0 = _mm256_max_pd(m_0, X_2);
        m_0 = _mm256_max_pd(m_0, X_3);
        m_0 = _mm256_max_pd(m_0, X_4);
        m_0 = _mm256_max_pd(m_0, X_5);
        m_0 = _mm256_max_pd(m_0, X_6);
        m_0 = _mm256_max_pd(m_0, X_7);
      }
      if(i + 16 <= N){
        X_0 = _mm256_and_pd(_mm256_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_loadu_pd(X + 4), abs_mask_tmp);
        X_2 = _mm256_and_pd(_mm256_loadu_pd(X + 8), abs_mask_tmp);
        X_3 = _mm256_and_pd(_mm256_loadu_pd(X + 12), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        m_0 = _mm256_max_pd(m_0, X_2);
        m_0 = _mm256_max_pd(m_0, X_3);
        i += 16, X += 16;
      }
      if(i + 8 <= N){
        X_0 = _mm256_and_pd(_mm256_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_loadu_pd(X + 4), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        i += 8, X += 8;
      }
      if(i + 4 <= N){
        X_0 = _mm256_and_pd(_mm256_loadu_pd(X), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        i += 4, X += 4;
      }
      if(i < N){
        X_0 = _mm256_and_pd(_mm256_set_pd(0, (N - i)>2?X[2]:0, (N - i)>1?X[1]:0, X[0]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        X += (N - i);
      }
    }else{

      for(i = 0; i + 32 <= N; i += 32, X += (incX * 32)){
        X_0 = _mm256_and_pd(_mm256_set_pd(X[(incX * 3)], X[(incX * 2)], X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_set_pd(X[(incX * 7)], X[(incX * 6)], X[(incX * 5)], X[(incX * 4)]), abs_mask_tmp);
        X_2 = _mm256_and_pd(_mm256_set_pd(X[(incX * 11)], X[(incX * 10)], X[(incX * 9)], X[(incX * 8)]), abs_mask_tmp);
        X_3 = _mm256_and_pd(_mm256_set_pd(X[(incX * 15)], X[(incX * 14)], X[(incX * 13)], X[(incX * 12)]), abs_mask_tmp);
        X_4 = _mm256_and_pd(_mm256_set_pd(X[(incX * 19)], X[(incX * 18)], X[(incX * 17)], X[(incX * 16)]), abs_mask_tmp);
        X_5 = _mm256_and_pd(_mm256_set_pd(X[(incX * 23)], X[(incX * 22)], X[(incX * 21)], X[(incX * 20)]), abs_mask_tmp);
        X_6 = _mm256_and_pd(_mm256_set_pd(X[(incX * 27)], X[(incX * 26)], X[(incX * 25)], X[(incX * 24)]), abs_mask_tmp);
        X_7 = _mm256_and_pd(_mm256_set_pd(X[(incX * 31)], X[(incX * 30)], X[(incX * 29)], X[(incX * 28)]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        m_0 = _mm256_max_pd(m_0, X_2);
        m_0 = _mm256_max_pd(m_0, X_3);
        m_0 = _mm256_max_pd(m_0, X_4);
        m_0 = _mm256_max_pd(m_0, X_5);
        m_0 = _mm256_max_pd(m_0, X_6);
        m_0 = _mm256_max_pd(m_0, X_7);
      }
      if(i + 16 <= N){
        X_0 = _mm256_and_pd(_mm256_set_pd(X[(incX * 3)], X[(incX * 2)], X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_set_pd(X[(incX * 7)], X[(incX * 6)], X[(incX * 5)], X[(incX * 4)]), abs_mask_tmp);
        X_2 = _mm256_and_pd(_mm256_set_pd(X[(incX * 11)], X[(incX * 10)], X[(incX * 9)], X[(incX * 8)]), abs_mask_tmp);
        X_3 = _mm256_and_pd(_mm256_set_pd(X[(incX * 15)], X[(incX * 14)], X[(incX * 13)], X[(incX * 12)]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        m_0 = _mm256_max_pd(m_0, X_2);
        m_0 = _mm256_max_pd(m_0, X_3);
        i += 16, X += (incX * 16);
      }
      if(i + 8 <= N){
        X_0 = _mm256_and_pd(_mm256_set_pd(X[(incX * 3)], X[(incX * 2)], X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm256_and_pd(_mm256_set_pd(X[(incX * 7)], X[(incX * 6)], X[(incX * 5)], X[(incX * 4)]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        m_0 = _mm256_max_pd(m_0, X_1);
        i += 8, X += (incX * 8);
      }
      if(i + 4 <= N){
        X_0 = _mm256_and_pd(_mm256_set_pd(X[(incX * 3)], X[(incX * 2)], X[incX], X[0]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        i += 4, X += (incX * 4);
      }
      if(i < N){
        X_0 = _mm256_and_pd(_mm256_set_pd(0, (N - i)>2?X[(incX * 2)]:0, (N - i)>1?X[incX]:0, X[0]), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, X_0);
        X += (incX * (N - i));
      }
    }
    _mm256_store_pd(max_buffer_tmp, m_0);
    max_buffer_tmp[0] = (max_buffer_tmp[0] > max_buffer_tmp[1] ? max_buffer_tmp[0]: max_buffer_tmp[1]);
    max_buffer_tmp[0] = (max_buffer_tmp[0] > max_buffer_tmp[2] ? max_buffer_tmp[0]: max_buffer_tmp[2]);
    max_buffer_tmp[0] = (max_buffer_tmp[0] > max_buffer_tmp[3] ? max_buffer_tmp[0]: max_buffer_tmp[3]);
    (&amax)[0] = max_buffer_tmp[0];

  #elif (defined(__SSE2__) && !defined(reproBLAS_no__SSE2__))
    __m128d abs_mask_tmp;
    {
      __m128d tmp;
      tmp = _mm_set1_pd(1);
      abs_mask_tmp = _mm_set1_pd(-1);
      abs_mask_tmp = _mm_xor_pd(abs_mask_tmp, tmp);
      tmp = _mm_cmpeq_pd(tmp, tmp);
      abs_mask_tmp = _mm_xor_pd(abs_mask_tmp, tmp);
    }
    double max_buffer_tmp[2] __attribute__((aligned(16))); (void)max_buffer_tmp;

    int i;

    __m128d X_0, X_1, X_2, X_3, X_4, X_5;
    __m128d m_0;
    m_0 = _mm_setzero_pd();

    if(incX == 1){

      for(i = 0; i + 12 <= N; i += 12, X += 12){
        X_0 = _mm_and_pd(_mm_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_loadu_pd(X + 2), abs_mask_tmp);
        X_2 = _mm_and_pd(_mm_loadu_pd(X + 4), abs_mask_tmp);
        X_3 = _mm_and_pd(_mm_loadu_pd(X + 6), abs_mask_tmp);
        X_4 = _mm_and_pd(_mm_loadu_pd(X + 8), abs_mask_tmp);
        X_5 = _mm_and_pd(_mm_loadu_pd(X + 10), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        m_0 = _mm_max_pd(m_0, X_2);
        m_0 = _mm_max_pd(m_0, X_3);
        m_0 = _mm_max_pd(m_0, X_4);
        m_0 = _mm_max_pd(m_0, X_5);
      }
      if(i + 8 <= N){
        X_0 = _mm_and_pd(_mm_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_loadu_pd(X + 2), abs_mask_tmp);
        X_2 = _mm_and_pd(_mm_loadu_pd(X + 4), abs_mask_tmp);
        X_3 = _mm_and_pd(_mm_loadu_pd(X + 6), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        m_0 = _mm_max_pd(m_0, X_2);
        m_0 = _mm_max_pd(m_0, X_3);
        i += 8, X += 8;
      }
      if(i + 4 <= N){
        X_0 = _mm_and_pd(_mm_loadu_pd(X), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_loadu_pd(X + 2), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        i += 4, X += 4;
      }
      if(i + 2 <= N){
        X_0 = _mm_and_pd(_mm_loadu_pd(X), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        i += 2, X += 2;
      }
      if(i < N){
        X_0 = _mm_and_pd(_mm_set_pd(0, X[0]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        X += (N - i);
      }
    }else{

      for(i = 0; i + 12 <= N; i += 12, X += (incX * 12)){
        X_0 = _mm_and_pd(_mm_set_pd(X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_set_pd(X[(incX * 3)], X[(incX * 2)]), abs_mask_tmp);
        X_2 = _mm_and_pd(_mm_set_pd(X[(incX * 5)], X[(incX * 4)]), abs_mask_tmp);
        X_3 = _mm_and_pd(_mm_set_pd(X[(incX * 7)], X[(incX * 6)]), abs_mask_tmp);
        X_4 = _mm_and_pd(_mm_set_pd(X[(incX * 9)], X[(incX * 8)]), abs_mask_tmp);
        X_5 = _mm_and_pd(_mm_set_pd(X[(incX * 11)], X[(incX * 10)]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        m_0 = _mm_max_pd(m_0, X_2);
        m_0 = _mm_max_pd(m_0, X_3);
        m_0 = _mm_max_pd(m_0, X_4);
        m_0 = _mm_max_pd(m_0, X_5);
      }
      if(i + 8 <= N){
        X_0 = _mm_and_pd(_mm_set_pd(X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_set_pd(X[(incX * 3)], X[(incX * 2)]), abs_mask_tmp);
        X_2 = _mm_and_pd(_mm_set_pd(X[(incX * 5)], X[(incX * 4)]), abs_mask_tmp);
        X_3 = _mm_and_pd(_mm_set_pd(X[(incX * 7)], X[(incX * 6)]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        m_0 = _mm_max_pd(m_0, X_2);
        m_0 = _mm_max_pd(m_0, X_3);
        i += 8, X += (incX * 8);
      }
      if(i + 4 <= N){
        X_0 = _mm_and_pd(_mm_set_pd(X[incX], X[0]), abs_mask_tmp);
        X_1 = _mm_and_pd(_mm_set_pd(X[(incX * 3)], X[(incX * 2)]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        m_0 = _mm_max_pd(m_0, X_1);
        i += 4, X += (incX * 4);
      }
      if(i + 2 <= N){
        X_0 = _mm_and_pd(_mm_set_pd(X[incX], X[0]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        i += 2, X += (incX * 2);
      }
      if(i < N){
        X_0 = _mm_and_pd(_mm_set_pd(0, X[0]), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, X_0);
        X += (incX * (N - i));
      }
    }
    _mm_store_pd(max_buffer_tmp, m_0);
    max_buffer_tmp[0] = (max_buffer_tmp[0] > max_buffer_tmp[1] ? max_buffer_tmp[0]: max_buffer_tmp[1]);
    (&amax)[0] = max_buffer_tmp[0];

  #else
    int i;

    double X_0;
    double m_0;
    m_0 = 0;

    if(incX == 1){

      for(i = 0; i + 1 <= N; i += 1, X += 1){
        X_0 = fabs(X[0]);
        m_0 = (m_0 > X_0? m_0: X_0);
      }
    }else{

      for(i = 0; i + 1 <= N; i += 1, X += incX){
        X_0 = fabs(X[0]);
        m_0 = (m_0 > X_0? m_0: X_0);
      }
    }
    (&amax)[0] = m_0;

  #endif
  //[[[end]]]
  return amax;
}
